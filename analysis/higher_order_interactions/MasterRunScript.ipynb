{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from skrebate import ReliefF # for reproducibility, need to install skrebate version 0.62\n",
    "from skrebate import SURF\n",
    "from skrebate import SURFstar\n",
    "from skrebate import MultiSURF\n",
    "from skrebate import MultiSURFstar\n",
    "from skrebate import TuRF\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function for directory maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(directory): # Check if directories exist. If they do not, they will be created in the next function\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for random shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shuffle(file_path):\n",
    "    # Define the directory to store results\n",
    "    results_dir = os.path.join(os.path.dirname(file_path), \"Results\", \"RandomShuffle\")\n",
    "    ensure_dir(results_dir)  # Ensure the Results/RandomShuffle directory exists\n",
    "\n",
    "    # Read the .txt file into a DataFrame\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='\\t')  # Assuming tab-separated values; adjust the separator as needed\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Ensure 'Class' column is not included in the shuffle\n",
    "    if 'Class' in df.columns:\n",
    "        columns_to_shuffle = df.drop('Class', axis=1).columns.tolist()\n",
    "    else:\n",
    "        columns_to_shuffle = df.columns.tolist()\n",
    "\n",
    "    # Shuffle the column names\n",
    "    shuffled_columns = np.random.permutation(columns_to_shuffle)\n",
    "\n",
    "    # Create a new DataFrame with the shuffled column names\n",
    "    new_df = pd.DataFrame(shuffled_columns, columns=['Feature'])\n",
    "\n",
    "    # Construct the output file name based on the original file's name\n",
    "    base_name = os.path.basename(file_path)\n",
    "    new_file_name = f\"{os.path.splitext(base_name)[0]}_RandShuffle.txt\"\n",
    "    output_path = os.path.join(results_dir, new_file_name)\n",
    "\n",
    "    # Write the new DataFrame to disk\n",
    "    new_df.to_csv(output_path, index=False, sep='\\t')\n",
    "\n",
    "def find_and_random_shuffle(root_dir):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        # Check if 'EDM' is in the directory name directly above the files\n",
    "        if \"EDM\" in os.path.basename(dirpath):\n",
    "            #print(f\"Processing in EDM directory: {dirpath}\")  # Debugging line\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.txt'):  # Ensure it's a .txt file\n",
    "                    file_path = os.path.join(dirpath, filename)\n",
    "                    #print(f\"Processing file: {file_path}\")  # Debugging line\n",
    "                    random_shuffle(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define current working directory (GAMETES_2.2_dev_peter_XOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'my_dir' # Change this to your parent directory path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run random shuffle process on directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through the directory and produce random assortments of the features. This is the Random Shuffle that will be presented in the paper\n",
    "find_and_random_shuffle(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mutual_info(file_path):\n",
    "    # Load the genetic data\n",
    "    genetic_data = pd.read_csv(file_path, sep='\\t')  # Ensure correct delimiter is used\n",
    "    \n",
    "    # Split the data\n",
    "    features, labels = genetic_data.drop('Class', axis=1).values, genetic_data['Class'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels)\n",
    "    \n",
    "    # Apply Mutual Information\n",
    "    mi_scores = mutual_info_classif(X_train, y_train)\n",
    "    \n",
    "    # Pair feature names with MI scores\n",
    "    temp_list = []\n",
    "    for feature_name, mi_score in zip(genetic_data.drop('Class', axis=1).columns, mi_scores):\n",
    "        temp_list.append([feature_name, mi_score])\n",
    "    \n",
    "    # Process results\n",
    "    Results = pd.DataFrame(temp_list, columns=['Feature', 'Feature_Importance'])\n",
    "    Results.sort_values(by='Feature_Importance', ascending=False, inplace=True)\n",
    "    \n",
    "    # Define directories\n",
    "    base_dir = os.path.dirname(file_path)\n",
    "    results_dir = os.path.join(base_dir, \"Results\")\n",
    "    mi_dir = os.path.join(results_dir, \"MutualInformation\")\n",
    "    ensure_dir(mi_dir)\n",
    "    \n",
    "    # Extract base filename without extension\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    # Save the files\n",
    "    Results.to_csv(os.path.join(mi_dir, f\"{base_name}_MIResults.txt\"), index=False, sep='\\t')\n",
    "\n",
    "def find_and_mutual_info(root_dir):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        # Check if 'EDM' is in the directory name directly above the files\n",
    "        if \"EDM\" in os.path.basename(dirpath):\n",
    "            #print(f\"Processing in EDM directory: {dirpath}\")  # Debugging line\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.txt'):  # Ensure it's a .txt file\n",
    "                    file_path = os.path.join(dirpath, filename)\n",
    "                    #print(f\"Processing file: {file_path}\")  # Debugging line\n",
    "                    process_mutual_info(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Mutual Information on directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through the directory and perform Mutual Information.\n",
    "find_and_mutual_info(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for ReliefF with 10 NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_relieff10(file_path):\n",
    "    # Load the genetic data\n",
    "    genetic_data = pd.read_csv(file_path, sep = '\\t')\n",
    "    \n",
    "    # Split the data\n",
    "    features, labels = genetic_data.drop('Class', axis=1).values, genetic_data['Class'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels)\n",
    "    \n",
    "    # Apply ReliefF\n",
    "    fs = ReliefF(n_features_to_select=2, n_neighbors=10)\n",
    "    fs.fit(X_train, y_train)\n",
    "    \n",
    "    temp_list = []\n",
    "    for feature_name, feature_score in zip(genetic_data.drop('Class', axis=1).columns, fs.feature_importances_):\n",
    "        temp_list.append([feature_name, feature_score])\n",
    "    \n",
    "    # Process results\n",
    "    Results = pd.DataFrame(temp_list, columns=['Feature', 'Feature_Importance'])\n",
    "    ABSResults = Results.copy()\n",
    "    ABSResults['ABS_Feature_Importance'] = ABSResults['Feature_Importance'].abs()\n",
    "    Results.sort_values(by='Feature_Importance', ascending=False, inplace=True)\n",
    "    ABSResults.sort_values(by='ABS_Feature_Importance', ascending=False, inplace=True)\n",
    "    \n",
    "    # Define directories\n",
    "    base_dir = os.path.dirname(file_path)\n",
    "    results_dir = os.path.join(base_dir, \"Results\")\n",
    "    relief_dir = os.path.join(results_dir, \"ReliefF10\")\n",
    "    abs_relief_dir = os.path.join(results_dir, \"ABS_ReliefF10\")\n",
    "    ensure_dir(relief_dir)\n",
    "    ensure_dir(abs_relief_dir)\n",
    "    \n",
    "    # Extract base filename without extension\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    # Save the files\n",
    "    Results.to_csv(os.path.join(relief_dir, f\"{base_name}_Results.txt\"), index=False, sep='\\t')\n",
    "    ABSResults.to_csv(os.path.join(abs_relief_dir, f\"{base_name}_ABSResults.txt\"), index=False, sep='\\t')\n",
    "\n",
    "def find_and_relieff10(root_dir):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "         # Check if 'EDM' is in the directory name directly above the files\n",
    "        if \"EDM\" in os.path.basename(dirpath):\n",
    "            #print(f\"Processing in EDM directory: {dirpath}\")  # Debugging line\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.txt'):  # Ensure it's a .txt file\n",
    "                    file_path = os.path.join(dirpath, filename)\n",
    "                    #print(f\"Processing file: {file_path}\")  # Debugging line\n",
    "                    process_relieff10(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ReliefF (10 NN) on directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_and_relieff10(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for ReliefF with 100 NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_relieff(file_path):\n",
    "    # Load the genetic data\n",
    "    genetic_data = pd.read_csv(file_path, sep = '\\t')\n",
    "    \n",
    "    # Split the data\n",
    "    features, labels = genetic_data.drop('Class', axis=1).values, genetic_data['Class'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels)\n",
    "    \n",
    "    # Apply ReliefF\n",
    "    fs = ReliefF(n_features_to_select=2, n_neighbors=100)\n",
    "    fs.fit(X_train, y_train)\n",
    "    \n",
    "    temp_list = []\n",
    "    for feature_name, feature_score in zip(genetic_data.drop('Class', axis=1).columns, fs.feature_importances_):\n",
    "        temp_list.append([feature_name, feature_score])\n",
    "    \n",
    "    # Process results\n",
    "    Results = pd.DataFrame(temp_list, columns=['Feature', 'Feature_Importance'])\n",
    "    ABSResults = Results.copy()\n",
    "    ABSResults['ABS_Feature_Importance'] = ABSResults['Feature_Importance'].abs()\n",
    "    Results.sort_values(by='Feature_Importance', ascending=False, inplace=True)\n",
    "    ABSResults.sort_values(by='ABS_Feature_Importance', ascending=False, inplace=True)\n",
    "    \n",
    "    # Define directories\n",
    "    base_dir = os.path.dirname(file_path)\n",
    "    results_dir = os.path.join(base_dir, \"Results\")\n",
    "    relief_dir = os.path.join(results_dir, \"ReliefF\")\n",
    "    abs_relief_dir = os.path.join(results_dir, \"ABS_ReliefF\")\n",
    "    ensure_dir(relief_dir)\n",
    "    ensure_dir(abs_relief_dir)\n",
    "    \n",
    "    # Extract base filename without extension\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    # Save the files\n",
    "    Results.to_csv(os.path.join(relief_dir, f\"{base_name}_Results.txt\"), index=False, sep='\\t')\n",
    "    ABSResults.to_csv(os.path.join(abs_relief_dir, f\"{base_name}_ABSResults.txt\"), index=False, sep='\\t')\n",
    "\n",
    "def find_and_relieff(root_dir):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "         # Check if 'EDM' is in the directory name directly above the files\n",
    "        if \"EDM\" in os.path.basename(dirpath):\n",
    "            #print(f\"Processing in EDM directory: {dirpath}\")  # Debugging line\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.txt'):  # Ensure it's a .txt file\n",
    "                    file_path = os.path.join(dirpath, filename)\n",
    "                    #print(f\"Processing file: {file_path}\")  # Debugging line\n",
    "                    process_relieff(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ReliefF (100 NN) on directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through the directory and perform ReliefF 100 NN.\n",
    "find_and_relieff(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for MultiSURF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_MultiSurf(file_path):\n",
    "    # Load the genetic data\n",
    "    genetic_data = pd.read_csv(file_path, sep = '\\t')\n",
    "    \n",
    "    # Split the data\n",
    "    features, labels = genetic_data.drop('Class', axis=1).values, genetic_data['Class'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels)\n",
    "    \n",
    "    # Apply MultiSurf\n",
    "    fs = MultiSURF()\n",
    "    fs.fit(X_train, y_train)\n",
    "    \n",
    "    temp_list = []\n",
    "    for feature_name, feature_score in zip(genetic_data.drop('Class', axis=1).columns, fs.feature_importances_):\n",
    "        temp_list.append([feature_name, feature_score])\n",
    "    \n",
    "    # Process results\n",
    "    Results = pd.DataFrame(temp_list, columns=['Feature', 'Feature_Importance'])\n",
    "    ABSResults = Results.copy()\n",
    "    ABSResults['ABS_Feature_Importance'] = ABSResults['Feature_Importance'].abs()\n",
    "    Results.sort_values(by='Feature_Importance', ascending=False, inplace=True)\n",
    "    ABSResults.sort_values(by='ABS_Feature_Importance', ascending=False, inplace=True)\n",
    "    \n",
    "    # Define directories\n",
    "    base_dir = os.path.dirname(file_path)\n",
    "    results_dir = os.path.join(base_dir, \"Results\")\n",
    "    multisurf_dir = os.path.join(results_dir, \"MultiSURF\")\n",
    "    abs_multisurf_dir = os.path.join(results_dir, \"ABS_MultiSURF\")\n",
    "    ensure_dir(multisurf_dir)\n",
    "    ensure_dir(abs_multisurf_dir)\n",
    "    \n",
    "    # Extract base filename without extension\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    # Save the files\n",
    "    Results.to_csv(os.path.join(multisurf_dir, f\"{base_name}_Results.txt\"), index=False, sep='\\t')\n",
    "    ABSResults.to_csv(os.path.join(abs_multisurf_dir, f\"{base_name}_ABSResults.txt\"), index=False, sep='\\t')\n",
    "\n",
    "def find_and_MultiSurf(root_dir):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "         # Check if 'EDM' is in the directory name directly above the files\n",
    "        if \"EDM\" in os.path.basename(dirpath):\n",
    "            #print(f\"Processing in EDM directory: {dirpath}\")  # Debugging line\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.txt'):  # Ensure it's a .txt file\n",
    "                    file_path = os.path.join(dirpath, filename)\n",
    "                    #print(f\"Processing file: {file_path}\")  # Debugging line\n",
    "                    process_MultiSurf(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MultiSURF on directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through the directory and perform MultiSURF.\n",
    "find_and_MultiSurf(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for MultiSURFstar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_MultiSurfstar(file_path):\n",
    "    # Load the genetic data\n",
    "    genetic_data = pd.read_csv(file_path, sep = '\\t')\n",
    "    \n",
    "    # Split the data\n",
    "    features, labels = genetic_data.drop('Class', axis=1).values, genetic_data['Class'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels)\n",
    "    \n",
    "    # Apply MultiSurf\n",
    "    fs = MultiSURFstar()\n",
    "    fs.fit(X_train, y_train)\n",
    "    \n",
    "    temp_list = []\n",
    "    for feature_name, feature_score in zip(genetic_data.drop('Class', axis=1).columns, fs.feature_importances_):\n",
    "        temp_list.append([feature_name, feature_score])\n",
    "    \n",
    "    # Process results\n",
    "    Results = pd.DataFrame(temp_list, columns=['Feature', 'Feature_Importance'])\n",
    "    ABSResults = Results.copy()\n",
    "    ABSResults['ABS_Feature_Importance'] = ABSResults['Feature_Importance'].abs()\n",
    "    Results.sort_values(by='Feature_Importance', ascending=False, inplace=True)\n",
    "    ABSResults.sort_values(by='ABS_Feature_Importance', ascending=False, inplace=True)\n",
    "    \n",
    "    # Define directories\n",
    "    base_dir = os.path.dirname(file_path)\n",
    "    results_dir = os.path.join(base_dir, \"Results\")\n",
    "    multisurfstar_dir = os.path.join(results_dir, \"MultiSURFstar\")\n",
    "    abs_multisurfstar_dir = os.path.join(results_dir, \"ABS_MultiSURFstar\")\n",
    "    ensure_dir(multisurfstar_dir)\n",
    "    ensure_dir(abs_multisurfstar_dir)\n",
    "    \n",
    "    # Extract base filename without extension\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    # Save the files\n",
    "    Results.to_csv(os.path.join(multisurfstar_dir, f\"{base_name}_Results.txt\"), index=False, sep='\\t')\n",
    "    ABSResults.to_csv(os.path.join(abs_multisurfstar_dir, f\"{base_name}_ABSResults.txt\"), index=False, sep='\\t')\n",
    "\n",
    "def find_and_MultiSurfstar(root_dir):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "         # Check if 'EDM' is in the directory name directly above the files\n",
    "        if \"EDM\" in os.path.basename(dirpath):\n",
    "            #print(f\"Processing in EDM directory: {dirpath}\")  # Debugging line\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.txt'):  # Ensure it's a .txt file\n",
    "                    file_path = os.path.join(dirpath, filename)\n",
    "                    #print(f\"Processing file: {file_path}\")  # Debugging line\n",
    "                    process_MultiSurfstar(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MultiSURFstar on directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through the first directory and perform MultiSURFstar.\n",
    "find_and_MultiSurfstar(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
